{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uv\n",
    "# !uv pip install git+https://github.com/ysenarath/AIT-724-HandsOn-2.git\n",
    "# !uv pip install --upgrade \"numpy==1.26.4\" \"scipy==1.12.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # for deep copying objects\n",
    "import gzip  # for handling gzip compressed files\n",
    "import os  # for operating system interactions\n",
    "import random  # for random number generation / sampling\n",
    "import shutil  # for high-level file operations\n",
    "import zipfile  # for handling zip files\n",
    "from datetime import timedelta  # for time duration representation\n",
    "from pathlib import Path  # for filesystem path manipulations\n",
    "from typing import Any, Callable, cast  # for type hinting\n",
    "\n",
    "from community import community_louvain  # for community detection\n",
    "import holoviews as hv  # for interactive visualizations\n",
    "import hvplot.networkx as hvnx  # for network visualizations\n",
    "import igraph as ig  # for faster graph analysis\n",
    "import matplotlib.pyplot as plt  # for static visualizations\n",
    "import networkx as nx  # for graph creation and manipulation\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import requests  # for downloading data\n",
    "import seaborn as sns  # for statistical data visualization\n",
    "from networkx.drawing.layout import forceatlas2_layout  # for ForceAtlas2 layout\n",
    "from networkx.drawing.nx_pydot import graphviz_layout  # for Graphviz layouts\n",
    "from tqdm import auto as tqdm  # for progress bars\n",
    "\n",
    "forceatlas2_layout = cast(\n",
    "    Callable[..., Any], forceatlas2_layout\n",
    ")  # type hinting workaround (you can ignore this line)\n",
    "\n",
    "hv.extension(\"bokeh\")  # initialize Holoviews with Bokeh backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_network(\n",
    "    G: nx.Graph,\n",
    "    edge_sample_size: int | None = None,\n",
    "    date_format: str = \"%Y-%m-%d %H:%M:%S\",\n",
    "    layout: str = \"forceatlas2\",\n",
    "    layout_kwargs: dict[str, Any] | None = None,\n",
    "    node_size: int = 10,\n",
    "    arrowhead_length: float = 0.01,\n",
    "):\n",
    "    # Utility function to visualize a NetworkX graph using hvplot\n",
    "    # Sample graph to manageable size\n",
    "    if edge_sample_size is not None and len(G.edges) > edge_sample_size:\n",
    "        # nodes_sample = random.sample(list(G.nodes()), sample_size)\n",
    "        # T = copy.deepcopy(G.subgraph(nodes_sample))\n",
    "        # sample edges and then select the subgraph induced by those edges\n",
    "        edges_sample = random.sample(list(G.edges()), edge_sample_size)\n",
    "        T = copy.deepcopy(G.edge_subgraph(edges_sample))\n",
    "    else:\n",
    "        T = copy.deepcopy(G)\n",
    "\n",
    "    # Convert Timestamp attributes to strings\n",
    "    for n, data in T.nodes(data=True):\n",
    "        for k, v in list(data.items()):\n",
    "            if isinstance(v, pd.Timestamp):\n",
    "                data[k] = v.strftime(date_format)\n",
    "\n",
    "    for u, v, data in T.edges(data=True):\n",
    "        for k, val in list(data.items()):\n",
    "            if isinstance(val, pd.Timestamp):\n",
    "                data[k] = val.strftime(date_format)\n",
    "\n",
    "    if layout == \"forceatlas2\":\n",
    "        default_kwargs = {\n",
    "            \"max_iter\": 100,\n",
    "            \"jitter_tolerance\": 1.0,\n",
    "            \"scaling_ratio\": 2.0,\n",
    "            \"gravity\": 1.0,\n",
    "        }\n",
    "        default_kwargs.update(layout_kwargs or {})\n",
    "        pos = forceatlas2_layout(T, **default_kwargs)\n",
    "    else:\n",
    "        pos = graphviz_layout(T, prog=layout or \"neato\")\n",
    "\n",
    "    # Create hvplot\n",
    "    plot = hvnx.draw(\n",
    "        T,\n",
    "        pos,\n",
    "        node_size=node_size,\n",
    "        node_color=\"lightblue\",\n",
    "        edge_color=\"gray\",\n",
    "        with_labels=True,\n",
    "        # arrow head size = 0.02,\n",
    "        arrows=True,\n",
    "        arrowhead_length=arrowhead_length,\n",
    "    ).opts(\n",
    "        width=900,\n",
    "        height=700,\n",
    "        tools=[\"hover\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"],\n",
    "        title=f\"Interactive Graph (|N|={len(T)}, |E|={len(T.edges)})\",\n",
    "    )\n",
    "\n",
    "    # Add hover info dynamically\n",
    "    hover_data = []\n",
    "    for n, d in T.nodes(data=True):\n",
    "        hover_data.append({\"id\": n, **d})\n",
    "        # You can attach hover info via node attributes if needed\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Files\n",
    "\n",
    "- The files are available at https://snap.stanford.edu/data/higgs-twitter.html\n",
    "\n",
    "- Base URL: https://snap.stanford.edu/data/higgs-*\n",
    "\n",
    "| File Name                   | Description                                                                                    |\n",
    "| --------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| social_network.edgelist.gz  | Friends/follower graph (directed)                                                              |\n",
    "| retweet_network.edgelist.gz | Graph of who retweets whom (directed and weighted)                                             |\n",
    "| reply_network.edgelist.gz   | Graph of who replies to who (directed and weighted)                                            |\n",
    "| mention_network.edgelist.gz | Graph of who mentions whom (directed and weighted)                                             |\n",
    "| higgs-activity_time.txt.gz  | The dataset provides information about activity on Twitter during the discovery of Higgs boson |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    local_filename = os.path.join(dest_folder, url.split(\"/\")[-1])\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        with (\n",
    "            open(local_filename, \"wb\") as f,\n",
    "            tqdm.tqdm(\n",
    "                desc=local_filename,\n",
    "                total=total_size,\n",
    "                unit=\"iB\",\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar,\n",
    "        ):\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "    elif zip_path.endswith(\".gz\"):\n",
    "        with gzip.open(zip_path, \"rb\") as f_in:\n",
    "            with open(os.path.splitext(zip_path)[0], \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://snap.stanford.edu/data/higgs-\"\n",
    "    files = [\n",
    "        \"social_network.edgelist.gz\",\n",
    "        \"retweet_network.edgelist.gz\",\n",
    "        \"reply_network.edgelist.gz\",\n",
    "        \"mention_network.edgelist.gz\",\n",
    "        \"activity_time.txt.gz\",\n",
    "    ]\n",
    "    dest_folder = \"higgs_twitter_data\"\n",
    "\n",
    "    for file_name in files:\n",
    "        url = base_url + file_name\n",
    "        output_path = Path(dest_folder) / (\"higgs-\" + file_name.replace(\".gz\", \"\"))\n",
    "        if output_path.exists():\n",
    "            print(f\"{file_name} already exists. Skipping download.\")\n",
    "            continue\n",
    "        print(f\"Downloading {url}...\")\n",
    "        downloaded_file = download_file(url, dest_folder)\n",
    "        print(f\"Unzipping {downloaded_file}...\")\n",
    "        unzip_file(downloaded_file, dest_folder)\n",
    "        os.remove(downloaded_file)  # Clean up the .gz file\n",
    "        print(f\"Finished processing {file_name}.\\n\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Edge List File Format\n",
    "\n",
    "- For this exercise, we will only explore the the activity time file: `higgs-activity_time.txt.gz`\n",
    "- This file contains four columns: userA userB timestamp interaction-type\n",
    "- Every edge is directed from userA to userB (e.g., userA replies to userB, this means information flows from userB to userA)\n",
    "- First let's try to load this file using pandas and explore its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = \"timestamp\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\"higgs_twitter_data\", \"higgs-activity_time.txt\"),\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=[\"source\", \"target\", TIMESTAMP, \"type\"],\n",
    ")\n",
    "\n",
    "print(f\"Total interactions in dataset: {len(df)}\")\n",
    "\n",
    "# Convert timestamp to datetime object\n",
    "df[TIMESTAMP] = pd.to_datetime(df[TIMESTAMP], unit=\"s\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=df, x=\"type\", discrete=True)\n",
    "plt.title(\"Distribution of Interaction Types\")\n",
    "plt.xlabel(\"Interaction Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"type\"] == \"RE\"]  # for the assignment this has to be changed\n",
    "start_date = df[TIMESTAMP].min()\n",
    "\n",
    "# If one is interested in building a network of how information flows,\n",
    "#   then the direction of RT should be reversed when used in the analysis.\n",
    "#   ref. https://snap.stanford.edu/data/higgs-twitter.html\n",
    "df = df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "\n",
    "total_number_of_nodes = df[[\"source\", \"target\"]].stack().nunique()\n",
    "total_number_of_edges = len(df)\n",
    "\n",
    "print(f\"Total number of unique nodes: {total_number_of_nodes}\")\n",
    "print(f\"Total number of edges: {total_number_of_edges}\")\n",
    "\n",
    "# let's look at the time range of the interactions\n",
    "min_time = df[TIMESTAMP].min()\n",
    "max_time = df[TIMESTAMP].max()\n",
    "print(f\"Time range of interactions: {min_time} to {max_time}\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G: nx.DiGraph = nx.from_pandas_edgelist(\n",
    "    df,\n",
    "    source=\"source\",\n",
    "    target=\"target\",\n",
    "    edge_attr=[\n",
    "        TIMESTAMP,\n",
    "        \"type\",  # type will always be \"RE\" here (or whatever we filtered for)\n",
    "    ],\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_network(G, edge_sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise diffusion: e.g., plot number of new retweeters over time.\n",
    "temp_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            TIMESTAMP: data[TIMESTAMP],\n",
    "            \"type\": data[\"type\"],\n",
    "        }\n",
    "        for u, v, data in G.edges(data=True)\n",
    "    ]\n",
    ")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    temp_df,\n",
    "    x=TIMESTAMP,\n",
    "    bins=50,\n",
    "    kde=False,\n",
    "    hue=\"type\",\n",
    "    multiple=\"stack\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree\n",
    "print(\"Computing node degrees...\")\n",
    "\n",
    "degree_dict = dict(G.degree())\n",
    "nx.set_node_attributes(G, degree_dict, \"degree\")\n",
    "\n",
    "# in degree\n",
    "in_degree_dict = dict(G.in_degree())\n",
    "nx.set_node_attributes(G, in_degree_dict, \"in_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "\n",
    "# # print(\"Computing betweenness centrality...\")\n",
    "# # takes around 15 mins-45mins\n",
    "# centrality_dict = nx.betweenness_centrality(G)\n",
    "# nx.set_node_attributes(G, centrality_dict, \"betweenness_centrality\")\n",
    "\n",
    "# follwing is a more efficient way using igraph, commented out for now\n",
    "\n",
    "# compute betweenness centrality for all vertices using igraph\n",
    "print(\"Computing betweenness centrality using igraph...\")\n",
    "G_ig = ig.Graph.from_networkx(G)\n",
    "vertex_betweenness = G_ig.betweenness()\n",
    "\n",
    "# put it back to networkx graph\n",
    "# betweenness_dict = {v.index: float(vertex_betweenness[v.index]) for v in G_ig.vs}\n",
    "names = [v[\"_nx_name\"] for v in G_ig.vs]\n",
    "betweenness_dict = dict(zip(names, vertex_betweenness))\n",
    "nx.set_node_attributes(G, betweenness_dict, \"betweenness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness centrality\n",
    "print(\"Computing closeness centrality...\")\n",
    "\n",
    "closeness_dict = nx.closeness_centrality(G)\n",
    "\n",
    "nx.set_node_attributes(G, closeness_dict, \"closeness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PageRank algorithm\n",
    "print(\"Computing PageRank...\")\n",
    "\n",
    "pagerank_dict = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "nx.set_node_attributes(G, pagerank_dict, \"pagerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering coefficient\n",
    "print(\"Computing clustering coefficient...\")\n",
    "\n",
    "clustering_dict = nx.clustering(G.to_undirected())\n",
    "\n",
    "nx.set_node_attributes(G, clustering_dict, \"clustering_coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add modularity class using the Louvain method\n",
    "partition = community_louvain.best_partition(G.to_undirected())\n",
    "\n",
    "for i, (key, value) in enumerate(partition.items()):\n",
    "    # check the random 5 nodes\n",
    "    print(f\"Node {key}: Modularity Class {value}\")\n",
    "    if i >= 4:\n",
    "        break\n",
    "\n",
    "nx.set_node_attributes(G, partition, \"modularity_class\")\n",
    "\n",
    "# print the number of nodes in each modularity class\n",
    "modularity_classes = pd.Series(partition).value_counts(sort=True, ascending=False)\n",
    "\n",
    "# top 5 largest classes\n",
    "print(\"Top 5 largest modularity classes:\")\n",
    "print(modularity_classes.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient=\"index\")\n",
    "\n",
    "# Follwing line can be used to see the distribution of modularity classes\n",
    "# temp_df[\"modularity_class\"].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common measure for quantifying influence of bloggers is to use in-degree centrality\n",
    "# - This is the number of users who follow a person on Twitter.\n",
    "# In-links are sparse\n",
    "# - More detailed analysis is required to measure influence\n",
    "\n",
    "in_degrees: dict[str, float] = nx.get_node_attributes(G, \"in_degree\")\n",
    "threshold = np.percentile(list(in_degrees.values()), 99)  # top 1%\n",
    "\n",
    "influencers = [n for n, deg in in_degrees.items() if deg >= threshold]\n",
    "regulars = [n for n in G.nodes if n not in influencers]\n",
    "\n",
    "print(f\"Influencers: {len(influencers)} | Regulars: {len(regulars)}\")\n",
    "\n",
    "# add this back to the network G\n",
    "influencer_dict = {n: (n in influencers) for n in G.nodes}\n",
    "nx.set_node_attributes(G, influencer_dict, \"is_influencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show node & edge attributes\n",
    "print(\"Node attribute names for first node:\")\n",
    "first_node = list(G.nodes)[0]\n",
    "print(G.nodes[first_node].keys())\n",
    "\n",
    "print(\"Edges attribute names for first node:\")\n",
    "first_edge = list(G.edges)[0]\n",
    "print(G.edges[first_edge].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_timestamps(\n",
    "    G: nx.DiGraph,\n",
    "    node: Any,\n",
    "    time_window: timedelta | None,\n",
    "    timestamp_col: str = \"timestamp\",\n",
    ") -> list[list[pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    We can assume that the interactions are about the same post if it happens within time_window.\n",
    "    We can use this function to group timestamps of edges (intractions) from a node (user)\n",
    "    into clusters based on time_window.\n",
    "\n",
    "    Example:\n",
    "    |       |   |      |                |       |\n",
    "    t1      t2  t3     t4               t5      t6\n",
    "\n",
    "    i0: [t1], [t2], [t3], [t4], [t5], [t6]\n",
    "\n",
    "    t2 - t1 < time_window, therefore merge\n",
    "    i0: [t1, t2], [t3], [t4], [t5], [t6]\n",
    "\n",
    "    t3 - t2 < time_window, therefore merge\n",
    "    i0: [t1, t2, t3], [t4], [t5], [t6]\n",
    "\n",
    "    t4 - t3 < time_window, therefore merge\n",
    "    i0: [t1, t2, t3, t4], [t5], [t6]\n",
    "\n",
    "    t5 - t4 >= time_window, therefore do not merge\n",
    "    i1: [t1, t2, t3, t4], [t5, t6]\n",
    "    \"\"\"\n",
    "    # Group timestamps of edges from a node into clusters based on time_window\n",
    "    # - first, get all timestamps of edges from the node\n",
    "    items = []\n",
    "    for source, target, data in G.edges(node, data=True):\n",
    "        dt = data[timestamp_col]\n",
    "        items.append(dt)\n",
    "    # - sort timestamps to ensure chronological order\n",
    "    items = sorted(items)\n",
    "    if time_window is None:\n",
    "        # - if no time window is specified, return all timestamps as a single group\n",
    "        groups = [items]\n",
    "    else:\n",
    "        # - cluster timestamps based on time_window\n",
    "        groups = [[x] for x in items]\n",
    "        i = 0\n",
    "        while i < len(groups) - 1:\n",
    "            # - if the time difference between two consecutive timestamps is less than time_window, merge them\n",
    "            # -- groups[i][-1] is the last timestamp in the current group\n",
    "            # -- groups[i + 1][0] is the first timestamp in the next group\n",
    "            if (groups[i + 1][0] - groups[i][-1]) < time_window:\n",
    "                groups[i].extend(groups[i + 1])\n",
    "                # - remove the next group after merging\n",
    "                # -- when groups[i + 1] is deleted, groups[i + 2] becomes the new groups[i + 1]\n",
    "                del groups[i + 1]\n",
    "                # - do not increment i, as we need to check the merged group with the new next one\n",
    "            else:\n",
    "                i += 1\n",
    "    return groups\n",
    "\n",
    "\n",
    "def build_cascade_from_root(\n",
    "    G: nx.DiGraph,\n",
    "    root: Any,\n",
    "    timestamp_cluster: list[pd.Timestamp],\n",
    "    visited: set,\n",
    "    time_window: timedelta | None = None,\n",
    "    timestamp_col: str = \"timestamp\",\n",
    ") -> nx.DiGraph:\n",
    "    # Start a cascade\n",
    "    cascade_nodes = set([root])\n",
    "    cascade_edges = dict()\n",
    "    queue = [(root, ts) for ts in timestamp_cluster]\n",
    "    visited.add(root)\n",
    "    while queue:\n",
    "        current, t_curr = queue.pop(0)\n",
    "        neighbors = sorted(\n",
    "            G.neighbors(current),\n",
    "            key=lambda nbr: G.get_edge_data(current, nbr, default={})[timestamp_col],\n",
    "        )\n",
    "        for nbr in neighbors:\n",
    "            edge = G.get_edge_data(current, nbr, default={})\n",
    "            # typeof datetime is pandas.Timestamp\n",
    "            t_edge = edge[timestamp_col]\n",
    "            # convert to real time and print\n",
    "            if time_window is not None and (t_edge - t_curr > time_window):\n",
    "                continue\n",
    "            if nbr not in visited:\n",
    "                cascade_nodes.add(nbr)\n",
    "                cascade_edges[(current, nbr)] = edge\n",
    "                queue.append((nbr, t_edge))\n",
    "                visited.add(nbr)\n",
    "    cascade = nx.DiGraph()\n",
    "    cascade.add_nodes_from(cascade_nodes)\n",
    "    cascade_edges_with_data = [\n",
    "        (src, tgt, data) for (src, tgt), data in cascade_edges.items()\n",
    "    ]\n",
    "    cascade.add_edges_from(cascade_edges_with_data)\n",
    "    return cascade\n",
    "\n",
    "\n",
    "def build_cascades(\n",
    "    G: nx.DiGraph,\n",
    "    time_window: timedelta | None = None,\n",
    "    min_cascade_size: int = 1,\n",
    "    timestamp_col: str = \"timestamp\",\n",
    ") -> list[nx.DiGraph]:\n",
    "    cascades = []\n",
    "    visited = set()\n",
    "    for root in G.nodes():\n",
    "        if root in visited:\n",
    "            continue\n",
    "        for timestamp_cluster in group_timestamps(\n",
    "            G, root, time_window=time_window, timestamp_col=timestamp_col\n",
    "        ):\n",
    "            cascade = build_cascade_from_root(\n",
    "                G,\n",
    "                root,\n",
    "                timestamp_cluster=timestamp_cluster,\n",
    "                visited=visited,\n",
    "                time_window=time_window,\n",
    "                timestamp_col=timestamp_col,\n",
    "            )\n",
    "            cascades.append(cascade)\n",
    "    # filter cascades with size > 1\n",
    "    cascades = [c for c in cascades if len(c) > min_cascade_size]\n",
    "    return cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades = build_cascades(G, time_window=None, min_cascade_size=10)\n",
    "print(f\"Total cascades found: {len(cascades)}\")\n",
    "\n",
    "largest_cascade = max(cascades, key=len)\n",
    "print(f\"Largest cascade size: {len(largest_cascade)}\")\n",
    "\n",
    "# size of the cascades\n",
    "cascade_sizes = [len(c) for c in cascades]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cascade_sizes, bins=30, kde=False)\n",
    "plt.title(\"Cascade Size Distribution\")\n",
    "plt.xlabel(\"Cascade Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades = build_cascades(G, time_window=timedelta(minutes=1), min_cascade_size=10)\n",
    "print(f\"Total cascades found: {len(cascades)}\")\n",
    "\n",
    "largest_cascade = max(cascades, key=len)\n",
    "print(f\"Largest cascade size: {len(largest_cascade)}\")\n",
    "\n",
    "# size of the cascades\n",
    "cascade_sizes = [len(c) for c in cascades]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cascade_sizes, bins=30, kde=False)\n",
    "plt.title(\"Cascade Size Distribution\")\n",
    "plt.xlabel(\"Cascade Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_network(largest_cascade, layout=\"twopi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Node data about node 677\n",
    "node_id = 677\n",
    "print(f\"Node {node_id} data:\")\n",
    "print(G.nodes[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the cascade sizes of influencers vs regular users\n",
    "# You may have to change the time_window parameter for the assignment.\n",
    "cascades = build_cascades(G, time_window=None, min_cascade_size=1)\n",
    "\n",
    "\n",
    "def get_roots(G: nx.DiGraph) -> list[Any]:\n",
    "    # roots are nodes with in-degree 0\n",
    "    roots = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "    return roots\n",
    "\n",
    "\n",
    "for cascade in cascades:\n",
    "    roots = get_roots(cascade)\n",
    "    if not roots:\n",
    "        continue\n",
    "    # get cascade size\n",
    "    cascade_size = len(cascade)\n",
    "    for root in roots:\n",
    "        if \"cascade_size\" in G.nodes[root]:\n",
    "            G.nodes[root][\"cascade_size\"] = max(\n",
    "                G.nodes[root][\"cascade_size\"], cascade_size\n",
    "            )\n",
    "        else:\n",
    "            G.nodes[root][\"cascade_size\"] = cascade_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient=\"index\")\n",
    "temp_df = temp_df.dropna(subset=[\"cascade_size\"])\n",
    "\n",
    "temp_df[\"is_influencer_yn\"] = temp_df[\"is_influencer\"].apply(\n",
    "    lambda x: \"Influencer\" if x else \"Regular\"\n",
    ")\n",
    "\n",
    "sns.boxplot(\n",
    "    data=temp_df,\n",
    "    x=\"is_influencer_yn\",\n",
    "    y=\"cascade_size\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Cascade Size by Influencer Status\")\n",
    "plt.xlabel(\"User Type\")\n",
    "plt.ylabel(\"Cascade Size (log scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

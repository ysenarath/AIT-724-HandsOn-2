{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uv\n",
    "# !uv pip install git+https://github.com/ysenarath/AIT-724-HandsOn-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import community as community_louvain\n",
    "import holoviews as hv\n",
    "import hvplot.networkx as hvnx\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from networkx.drawing.layout import forceatlas2_layout as _forceatlas2_layout\n",
    "from tqdm import auto as tqdm\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "\n",
    "def forceatlas2_layout(*args, **kwargs) -> dict:\n",
    "    return _forceatlas2_layout(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Files\n",
    "\n",
    "- The files are available at https://snap.stanford.edu/data/higgs-twitter.html\n",
    "\n",
    "- Base URL: https://snap.stanford.edu/data/higgs-*\n",
    "\n",
    "| File Name                   | Description                                                                                    |\n",
    "| --------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| social_network.edgelist.gz  | Friends/follower graph (directed)                                                              |\n",
    "| retweet_network.edgelist.gz | Graph of who retweets whom (directed and weighted)                                             |\n",
    "| reply_network.edgelist.gz   | Graph of who replies to who (directed and weighted)                                            |\n",
    "| mention_network.edgelist.gz | Graph of who mentions whom (directed and weighted)                                             |\n",
    "| higgs-activity_time.txt.gz  | The dataset provides information about activity on Twitter during the discovery of Higgs boson |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    local_filename = os.path.join(dest_folder, url.split(\"/\")[-1])\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        with (\n",
    "            open(local_filename, \"wb\") as f,\n",
    "            tqdm.tqdm(\n",
    "                desc=local_filename,\n",
    "                total=total_size,\n",
    "                unit=\"iB\",\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar,\n",
    "        ):\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "    elif zip_path.endswith(\".gz\"):\n",
    "        with gzip.open(zip_path, \"rb\") as f_in:\n",
    "            with open(os.path.splitext(zip_path)[0], \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://snap.stanford.edu/data/higgs-\"\n",
    "    files = [\n",
    "        \"social_network.edgelist.gz\",\n",
    "        \"retweet_network.edgelist.gz\",\n",
    "        \"reply_network.edgelist.gz\",\n",
    "        \"mention_network.edgelist.gz\",\n",
    "        \"activity_time.txt.gz\",\n",
    "    ]\n",
    "    dest_folder = \"higgs_twitter_data\"\n",
    "\n",
    "    for file_name in files:\n",
    "        url = base_url + file_name\n",
    "        output_path = Path(dest_folder) / (\"higgs-\" + file_name.replace(\".gz\", \"\"))\n",
    "        if output_path.exists():\n",
    "            print(f\"{file_name} already exists. Skipping download.\")\n",
    "            continue\n",
    "        print(f\"Downloading {url}...\")\n",
    "        downloaded_file = download_file(url, dest_folder)\n",
    "        print(f\"Unzipping {downloaded_file}...\")\n",
    "        unzip_file(downloaded_file, dest_folder)\n",
    "        os.remove(downloaded_file)  # Remove the compressed file after extraction\n",
    "        print(f\"Finished processing {file_name}.\\n\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Edge List File Format\n",
    "\n",
    "- For this exercise, we will only explore the social network (followers/friends) edge list file: `social_network.edgelist.gz` and the activity time file: `higgs-activity_time.txt.gz`\n",
    "- First we extract the largest connected component from the social network and use this subgraph for our analysis.\n",
    "- This file contains four columns: userA userB timestamp interaction type\n",
    "- every edge is directed from userA to userB\n",
    "- First let's try to load this file using pandas and explore its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_event_date = pd.to_datetime(\"2012-07-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a network graph using NetworkX\n",
    "follower_df = pd.read_csv(\n",
    "    os.path.join(\"higgs_twitter_data\", \"higgs-social_network.edgelist\"),\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=[\"source\", \"target\"],\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Create a directed graph from the follower data\n",
    "F: nx.DiGraph = nx.from_pandas_edgelist(\n",
    "    follower_df,\n",
    "    source=\"source\",\n",
    "    target=\"target\",\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "\n",
    "# Let's identify the largest weakly connected component (LCC) for the follower graph\n",
    "largest_wcc = max(nx.weakly_connected_components(F), key=len)\n",
    "\n",
    "F_wcc = F.subgraph(largest_wcc)\n",
    "\n",
    "print(\n",
    "    f\"Follower graph LCC has {F_wcc.number_of_nodes()} nodes and {F_wcc.number_of_edges()} edges.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 nodes in the LCC\n",
    "[n for i, n in enumerate(F_wcc.nodes()) if i < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(\"higgs_twitter_data\", \"higgs-activity_time.txt\"),\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=[\"source\", \"target\", \"timestamp\", \"type\"],\n",
    ")\n",
    "print(f\"Total interactions in dataset: {len(df)}\")\n",
    "\n",
    "# filter interactions where both source and target are in the follower LCC\n",
    "df = df[\n",
    "    (df[\"source\"].isin(largest_wcc)) & (df[\"target\"].isin(largest_wcc))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total interactions after filtering to follower LCC: {len(df)}\")\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "# let's look at the distribution of interaction types\n",
    "interaction_counts = df[\"type\"].value_counts()\n",
    "print(\"Interaction type distribution:\")\n",
    "print(interaction_counts)\n",
    "\n",
    "# Get {DT} days before and after the main event date\n",
    "# DT = 1\n",
    "# start_date = main_event_date - pd.Timedelta(days=DT)\n",
    "# end_date = main_event_date + pd.Timedelta(days=DT)\n",
    "# df = df[(df[\"datetime\"] >= start_date) & (df[\"datetime\"] <= end_date)]\n",
    "df = df[df[\"type\"] == \"RE\"]\n",
    "start_date = df[\"datetime\"].min()\n",
    "\n",
    "# If one is interested in building a network of how information flows, then the direction of RT should be reversed when used in the analysis.\n",
    "#   ref. https://snap.stanford.edu/data/higgs-twitter.html\n",
    "df = df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "\n",
    "total_number_of_nodes = df[[\"source\", \"target\"]].stack().nunique()\n",
    "total_number_of_edges = len(df)\n",
    "\n",
    "print(f\"Total number of unique nodes: {total_number_of_nodes}\")\n",
    "print(f\"Total number of edges: {total_number_of_edges}\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the time range of the interactions\n",
    "min_time = df[\"datetime\"].min()\n",
    "max_time = df[\"datetime\"].max()\n",
    "print(f\"Time range of interactions: {min_time} to {max_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G: nx.DiGraph = nx.from_pandas_edgelist(\n",
    "    df,\n",
    "    source=\"source\",\n",
    "    target=\"target\",\n",
    "    edge_attr=[\"datetime\", \"type\"],\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the graph with matplotlib for the first hour of data\n",
    "\n",
    "sampled_df = df[(start_date <= df[\"datetime\"]) & (df[\"datetime\"] < main_event_date)]\n",
    "\n",
    "H: nx.DiGraph = nx.from_pandas_edgelist(\n",
    "    sampled_df,\n",
    "    source=\"source\",\n",
    "    target=\"target\",\n",
    "    edge_attr=[\"datetime\", \"type\"],\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "pos = forceatlas2_layout(\n",
    "    H,\n",
    "    max_iter=20,  # default is 100, higher values give more precise layout\n",
    "    jitter_tolerance=0.5,  # default is 1.0, lower is more precise\n",
    "    scaling_ratio=3.0,  # default is 2.0, higher values spread out the layout\n",
    "    gravity=0.5,  # default is 1.0, higher values pull nodes towards the center\n",
    ")\n",
    "\n",
    "nx.draw(H, pos, with_labels=True, node_size=50, font_size=12)\n",
    "\n",
    "plt.title(\"Higgs Twitter Network - First Hour of Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise early diffusion: e.g., plot number of new retweeters over time.\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    df,\n",
    "    x=\"datetime\",\n",
    "    bins=50,\n",
    "    kde=False,\n",
    "    hue=\"type\",\n",
    "    multiple=\"stack\",\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree\n",
    "print(\"Computing node degrees...\")\n",
    "degree_dict = dict(G.degree())\n",
    "nx.set_node_attributes(G, degree_dict, \"degree\")\n",
    "\n",
    "# in degree\n",
    "in_degree_dict = dict(G.in_degree())\n",
    "nx.set_node_attributes(G, in_degree_dict, \"in_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "\n",
    "# # print(\"Computing betweenness centrality...\")\n",
    "# # takes around 15 mins-45mins\n",
    "# centrality_dict = nx.betweenness_centrality(G)\n",
    "# nx.set_node_attributes(G, centrality_dict, \"betweenness_centrality\")\n",
    "\n",
    "# follwing is a more efficient way using igraph, commented out for now\n",
    "\n",
    "# compute betweenness centrality for all vertices using igraph\n",
    "print(\"Computing betweenness centrality using igraph...\")\n",
    "G_ig = ig.Graph.from_networkx(G)\n",
    "vertex_betweenness = G_ig.betweenness()\n",
    "\n",
    "# put it back to networkx graph\n",
    "# betweenness_dict = {v.index: float(vertex_betweenness[v.index]) for v in G_ig.vs}\n",
    "names = [v[\"_nx_name\"] for v in G_ig.vs]\n",
    "betweenness_dict = dict(zip(names, vertex_betweenness))\n",
    "nx.set_node_attributes(G, betweenness_dict, \"betweenness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness centrality\n",
    "print(\"Computing closeness centrality...\")\n",
    "\n",
    "closeness_dict = nx.closeness_centrality(G)\n",
    "\n",
    "nx.set_node_attributes(G, closeness_dict, \"closeness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PageRank algorithm\n",
    "print(\"Computing PageRank...\")\n",
    "\n",
    "pagerank_dict = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "nx.set_node_attributes(G, pagerank_dict, \"pagerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering coefficient\n",
    "print(\"Computing clustering coefficient...\")\n",
    "\n",
    "clustering_dict = nx.clustering(G.to_undirected())\n",
    "\n",
    "nx.set_node_attributes(G, clustering_dict, \"clustering_coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add modularity class using the Louvain method\n",
    "partition = community_louvain.best_partition(G.to_undirected())\n",
    "\n",
    "nx.set_node_attributes(G, partition, \"modularity_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common measure for quantifying influence of bloggers is to use in-degree centrality\n",
    "# - This is the number of users who follow a person on Twitter.\n",
    "# In-links are sparse\n",
    "# - More detailed analysis is required to measure influence\n",
    "\n",
    "in_degrees: dict[str, float] = nx.get_node_attributes(G, \"in_degree\")\n",
    "threshold = np.percentile(list(in_degrees.values()), 99)  # top 1%\n",
    "\n",
    "influencers = [n for n, deg in in_degrees.items() if deg >= threshold]\n",
    "regulars = [n for n in G.nodes if n not in influencers]\n",
    "\n",
    "print(f\"Influencers: {len(influencers)} | Regulars: {len(regulars)}\")\n",
    "\n",
    "# add this back to the network G\n",
    "influencer_dict = {n: n in influencers for n in G.nodes}\n",
    "nx.set_node_attributes(G, influencer_dict, \"is_influencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show node & edge attributes\n",
    "print(\"Node attribute names for first node:\")\n",
    "first_node = list(G.nodes)[0]\n",
    "print(G.nodes[first_node].keys())\n",
    "print(\"Edges attribute names for first node:\")\n",
    "first_edge = list(G.edges)[0]\n",
    "print(G.edges[first_edge].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_timestamps(\n",
    "    G: nx.DiGraph,\n",
    "    node: Any,\n",
    "    time_window: timedelta | None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> list[list[pd.Timestamp]]:\n",
    "    items = []\n",
    "    for source, target, data in G.edges(node, data=True):\n",
    "        dt = data[timestamp_col]\n",
    "        items.append(dt)\n",
    "    items = sorted(items)\n",
    "    if time_window is None:\n",
    "        # all in one cluster\n",
    "        clusters = [[items]]\n",
    "    else:\n",
    "        clusters = [[x] for x in items]\n",
    "        i = 0\n",
    "        while i < len(clusters) - 1:\n",
    "            if (clusters[i + 1][0] - clusters[i][0]) < time_window:\n",
    "                clusters[i].extend(clusters[i + 1])\n",
    "                del clusters[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def build_cascade_from_root(\n",
    "    G: nx.DiGraph,\n",
    "    root: Any,\n",
    "    timestamp_cluster: list[pd.Timestamp],\n",
    "    visited: set,\n",
    "    time_window: timedelta | None = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> nx.DiGraph:\n",
    "    # Start a cascade\n",
    "    cascade_nodes = set([root])\n",
    "    cascade_edges = dict()\n",
    "    queue = [(root, ts) for ts in timestamp_cluster]\n",
    "    visited.add(root)\n",
    "    while queue:\n",
    "        current, t_curr = queue.pop(0)\n",
    "        neighbors = sorted(\n",
    "            G.neighbors(current),\n",
    "            key=lambda nbr: G.get_edge_data(current, nbr, default={})[timestamp_col],\n",
    "        )\n",
    "        for nbr in neighbors:\n",
    "            edge = G.get_edge_data(current, nbr, default={})\n",
    "            # typeof datetime is pandas.Timestamp\n",
    "            t_edge = edge[timestamp_col]\n",
    "            # convert to real time and print\n",
    "            if time_window is not None and (t_edge - t_curr > time_window):\n",
    "                continue\n",
    "            if nbr not in visited:\n",
    "                cascade_nodes.add(nbr)\n",
    "                cascade_edges[(current, nbr)] = edge\n",
    "                queue.append((nbr, t_edge))\n",
    "                visited.add(nbr)\n",
    "    cascade = nx.DiGraph()\n",
    "    cascade.add_nodes_from(cascade_nodes)\n",
    "    cascade_edges_with_data = [\n",
    "        (src, tgt, data) for (src, tgt), data in cascade_edges.items()\n",
    "    ]\n",
    "    cascade.add_edges_from(cascade_edges_with_data)\n",
    "    return cascade\n",
    "\n",
    "\n",
    "def build_cascades(\n",
    "    G: nx.DiGraph,\n",
    "    time_window: timedelta | None = None,\n",
    "    min_cascade_size: int = 1,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> list[nx.DiGraph]:\n",
    "    cascades = []\n",
    "    visited = set()\n",
    "    for root in G.nodes():\n",
    "        if root in visited:\n",
    "            continue\n",
    "        for timestamp_cluster in group_timestamps(\n",
    "            G, root, time_window=time_window, timestamp_col=timestamp_col\n",
    "        ):\n",
    "            cascade = build_cascade_from_root(\n",
    "                G,\n",
    "                root,\n",
    "                timestamp_cluster=timestamp_cluster,\n",
    "                visited=visited,\n",
    "                time_window=time_window,\n",
    "                timestamp_col=timestamp_col,\n",
    "            )\n",
    "            cascades.append(cascade)\n",
    "    # filter cascades with size > 1\n",
    "    cascades = [c for c in cascades if len(c) > min_cascade_size]\n",
    "    return cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades = build_cascades(G, time_window=timedelta(minutes=1), min_cascade_size=10)\n",
    "print(f\"Total cascades found: {len(cascades)}\")\n",
    "largest_cascade = max(cascades, key=len)\n",
    "print(f\"Largest cascade size: {len(largest_cascade)}\")\n",
    "# size of the cascades\n",
    "cascade_sizes = [len(c) for c in cascades]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cascade_sizes, bins=30, kde=False)\n",
    "plt.title(\"Cascade Size Distribution\")\n",
    "plt.xlabel(\"Cascade Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades = build_cascades(G, time_window=None, min_cascade_size=10)\n",
    "print(f\"Total cascades found: {len(cascades)}\")\n",
    "largest_cascade = max(cascades, key=len)\n",
    "print(f\"Largest cascade size: {len(largest_cascade)}\")\n",
    "# size of the cascades\n",
    "cascade_sizes = [len(c) for c in cascades]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cascade_sizes, bins=30, kde=False)\n",
    "plt.title(\"Cascade Size Distribution\")\n",
    "plt.xlabel(\"Cascade Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_network_hvplot(\n",
    "    G: nx.Graph, sample_size: int | None = None, date_format: str = \"%Y-%m-%d\"\n",
    "):\n",
    "    # Sample graph to manageable size\n",
    "    if sample_size is not None and len(G.nodes) > sample_size:\n",
    "        nodes_sample = random.sample(list(G.nodes()), sample_size)\n",
    "        G_copy = copy.deepcopy(G.subgraph(nodes_sample))\n",
    "    else:\n",
    "        G_copy = copy.deepcopy(G)\n",
    "\n",
    "    # Convert Timestamp attributes to strings\n",
    "    for n, data in G_copy.nodes(data=True):\n",
    "        for k, v in list(data.items()):\n",
    "            if isinstance(v, pd.Timestamp):\n",
    "                data[k] = v.strftime(date_format)\n",
    "\n",
    "    for u, v, data in G_copy.edges(data=True):\n",
    "        for k, val in list(data.items()):\n",
    "            if isinstance(val, pd.Timestamp):\n",
    "                data[k] = val.strftime(date_format)\n",
    "\n",
    "    # Compute layout (spring layout works well for most graphs)\n",
    "    pos = forceatlas2_layout(\n",
    "        G_copy,\n",
    "        max_iter=800,\n",
    "        jitter_tolerance=1.0,\n",
    "        scaling_ratio=2.0,\n",
    "        gravity=1.0,\n",
    "    )\n",
    "\n",
    "    # Create hvplot\n",
    "    plot = hvnx.draw(\n",
    "        G_copy,\n",
    "        pos,\n",
    "        node_size=100,\n",
    "        node_color=\"lightblue\",\n",
    "        edge_color=\"gray\",\n",
    "        with_labels=True,\n",
    "    ).opts(\n",
    "        width=900,\n",
    "        height=700,\n",
    "        tools=[\"hover\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"],\n",
    "        title=f\"Interactive Graph (|N|={len(G_copy)}, |E|={len(G_copy.edges)})\",\n",
    "    )\n",
    "\n",
    "    # Add hover info dynamically\n",
    "    hover_data = []\n",
    "    for n, d in G_copy.nodes(data=True):\n",
    "        hover_data.append({\"id\": n, **d})\n",
    "        # You can attach hover info via node attributes if needed\n",
    "\n",
    "    return plot\n",
    "\n",
    "\n",
    "show_network_hvplot(cascades[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
